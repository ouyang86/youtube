{
 "metadata": {
  "name": "",
  "signature": "sha256:0b246dddb1b68df74b809a0f804ae31beba221f46cadd142c2f53a5ad9028fee"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem.snowball import SnowballStemmer\n",
      "from math import log, exp\n",
      "import json, urllib, re, nltk\n",
      "import cPickle as pickle\n",
      "import gc\n",
      "\n",
      "DEVELOPER_KEY = \"AIzaSyBdLy0yxLKo-Az9oDY4RTb2-IjllcBtgQY\"\n",
      "service_url = 'https://www.googleapis.com/freebase/v1/topic'\n",
      "punc_pair = {u'[':u']', u'\"': u'\"', u'(':u')', u'{': u'}', u'\uff08': u'\uff09',\n",
      "             u'\u300a': u'\u300b', u'\u3010': u'\u3011', u'\u3016': u'\u3017', u'\u300c':u'\u300d',\n",
      "             u'\u300e':u'\u300f', u'\u201c':u'\u201d', u'\u00ab':u'\u00bb', u\"'\":u\"'\", u\"\u263c\":u\"\u263c\",\n",
      "             u'\u301c':u'\u301c', u'\u25c4':u'\u25ba', u'':u''}\n",
      "\n",
      "states = ['SONG', 'ARTIST', 'OTHER', 'FEAT', 'SEP', 'JOIN']\n",
      "ftoj = pickle.load(open('ftoj.p', 'rb'))\n",
      "emit_p = pickle.load(open('emit_p.p', 'rb'))\n",
      "trans_p = pickle.load(open('tran_p.p', 'rb'))\n",
      "start_p = pickle.load(open('start_p.p', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sqlite3\n",
      "def tokenize(text, avoid=False):    \n",
      "    token_list = re.split('\\s+', text)\n",
      "    output = []\n",
      "    web = ur\"(?:https?://|www.)[\\w\\u00C0-\\u00FF]+(?:[./?=-][\\w\\u00C0-\\u00FF]+)*[/]?\"\n",
      "    special = ur\"(?:[A-Z]?[\\w$&][/.])+(?:[\\w$&])*\"\n",
      "    date = ur\"(?:[\\d]+[/.])+[\\d]+\"\n",
      "    song = ur\"[Ff](?:[Ee][Aa])?[Tt]\\.|[Vv][Ee][Rr]\\.|[Pp]t\\.|[Nn]o\\.|[Oo]p\\.|Mr\\.|Ms\\.|Jr\\.|Dr\\.|[Vv]s\\.\"\n",
      "    latin1 = ur\"[$\\w\\u00C0-\\u00FF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]+\"\n",
      "    latin2 = ur\"(?:[-'\\u2018\\u2019][$\\w\\u00C0-\\u00FF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]+)*|[.]{3}\"\n",
      "    latin_scripts = latin1 + latin2\n",
      "    latin_punc = ur'[\\u0021-\\u002F]|[\\u003A-\\u0040]|[\\u005B-\\u0060]|[\\u007B-\\u007E]|[\\u00A1-\\u00BF]'\n",
      "    cjk_scripts = ur'[\\u4E00-\\u9FFF]|[\\u30A0-\\u30FF]+|[\\u3040-\\u309F]+|[\\uAC00-\\uD7AF]+'\n",
      "    cjk_punc = ur'[\\u3000-\\u303F]|[\\uFF00-\\uFFEF]'\n",
      "    cyr_scripts = ur\"[\\d\\u0400-\\u04FF]+(?:[-.'\\u2018\\u2019][\\d\\u0400-\\u04FF]+)*\"\n",
      "    cyr_special = ur\"(?:[\u0410-\u042f\\d&]+\\.)+\"\n",
      "    sa_scripts = ur'[\\u0900-\\u097F]+|[\\uA8E0-\\uA8FF]+|[\\u1CD0-\\u1CFF]+|[\\u0980-\\u09FF]+'\n",
      "    arb_scripts = ur'[\\u0600-\\u06FF\\u0750-\\u077F]+'\n",
      "    gre_scripts = ur'[\\u0370-\\u03FF\\u1F00-\\u1FFF]+'\n",
      "    gen_punc = ur'[\\u2000-\\u206F]'\n",
      "    mis_symbols = ur'[\\u2600-\\u26FF\\u25A0-\\u25FF\\u2192]'\n",
      "    all_scripts = '|'.join([web, date, special, song, latin_scripts, latin_punc, cjk_scripts, cjk_punc, cyr_scripts,\n",
      "                            cyr_special, sa_scripts, arb_scripts, gre_scripts, gen_punc, mis_symbols])\n",
      "    pattern = re.compile(all_scripts)\n",
      "    for token in token_list:\n",
      "        subtoken_list = re.findall(pattern, token)\n",
      "        for subtoken in subtoken_list:\n",
      "            output.append(subtoken)\n",
      "    return output\n",
      "\n",
      "\n",
      "#stem a token (not a whole sentence) from a tokenizer\n",
      "def stem(token):\n",
      "    latin = ur\"[\\u00C0-\\u00FF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]\"\n",
      "    jk_scripts = ur'[\\u30A0-\\u30FF]|[\\u3040-\\u309F]|[\\uAC00-\\uD7AF]'\n",
      "    ch_scripts = ur'[\\u4E00-\\u9FFF]'\n",
      "    cjk_punc = ur'[\\u3000-\\u303F]|[\\uFF00-\\uFFEF]'\n",
      "    cyr_scripts = ur'[\\u0400-\\u04FF]'\n",
      "    sa_scripts = ur'[\\u0900-\\u097F]|[\\uA8E0-\\uA8FF]|[\\u1CD0-\\u1CFF]|[\\u0980-\\u09FF]'\n",
      "    arb_scripts = ur'[\\u0600-\\u06FF\\u0750-\\u077F]'\n",
      "    gre_scripts = ur'[\\u0370-\\u03FF\\u1F00-\\u1FFF]'\n",
      "    gen_punc = ur'[\\u2000-\\u206F]'\n",
      "    mis_symbols = ur'[\\u2600-\\u26FF]'\n",
      "    feat = ur'featur(?:e|ing)|f(?:ea)?t[.]?'\n",
      "    ver = ur'ver(?:\\.|sion)'\n",
      "    vs = ur'v(?:ersu)?[.]?s[.]?'\n",
      "    non_eng = '|'.join([latin, jk_scripts, cjk_punc, sa_scripts, arb_scripts, \n",
      "                        gre_scripts, gen_punc, mis_symbols])\n",
      "    non_eng_pattern = re.compile(non_eng)\n",
      "    ru_pattern = re.compile(cyr_scripts)\n",
      "    ch_pattern = re.compile(ch_scripts)\n",
      "    quotation = re.compile(ur\"[\\u2018\\u2019]\")\n",
      "    comma = re.compile(ur\"\uff0c\")\n",
      "    aand = re.compile(ur\"[&+]+\")\n",
      "    feat_pattern = re.compile(feat, re.IGNORECASE)\n",
      "    ver_pattern = re.compile(ver, re.IGNORECASE)\n",
      "    vs_pattern = re.compile(vs, re.IGNORECASE)\n",
      "    hyphen_pattern = re.compile(ur\"[\\uff0d\\u2013\\u2192]\", \n",
      "                                re.IGNORECASE)\n",
      "    token = re.sub(quotation, u\"'\", token)\n",
      "    token= re.sub(comma, u\",\", token)\n",
      "    #token = re.sub(aand, u\"&\", token)\n",
      "    token = re.sub(feat_pattern, u\"ft\", token)\n",
      "    token = re.sub(ver_pattern, u\"ver\", token)\n",
      "    token = re.sub(vs_pattern, u\"vs\", token)\n",
      "    token = re.sub(hyphen_pattern, u'-', token)\n",
      "    if re.search(ch_pattern, token):\n",
      "        return ch_stem(token)\n",
      "    if re.search(non_eng_pattern, token):\n",
      "        return token.lower()\n",
      "    if re.search(ru_pattern, token):\n",
      "        return SnowballStemmer('russian').stem(token)\n",
      "    return SnowballStemmer('english').stem(token)\n",
      "\n",
      "\n",
      "#transform any Chinese string into Simplified Chinese\n",
      "def ch_stem(text):\n",
      "    result = ''\n",
      "    for char in text:\n",
      "        stem_char = ftoj.get(char, char)\n",
      "        result += stem_char\n",
      "    return result\n",
      "\n",
      "\n",
      "#training for artist name using MusicBrianz data\n",
      "def train_artist(filepath, fields, weights,seperator,splitor,\n",
      "                 normalize=False, logalize=False):\n",
      "    result = {}\n",
      "    avoid_token_list = set([u\"part\", u\"pt.\", u\"no.\", u\"op.\", u\"vs.\",u\"vs\",\n",
      "                        u\"feat\", u\"feat.\", u\"ft.\", u\"ft\",  u\"-\", u\"'\",\n",
      "                        u\"bwv\", u\"k.\"])\n",
      "    special = ur\"(?:[A-Z]?[\\w$&][/.])+(?:[\\w$&])*\"\n",
      "    date = ur\"(?:[\\d]+[/.])+[\\d]+\"\n",
      "    song = ur\"[Ff](?:[Ee][Aa])?[Tt]\\.|[Vv][Ee][Rr]\\.|[Pp]t\\.|[Nn]o\\.|[Oo]p\\.|Mr\\.|Ms\\.|Jr\\.|Dr\\.|[Vv]s\\.\"\n",
      "    latin1 = ur\"[$\\w\\u00C0-\\u00FF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]+\"\n",
      "    latin2 = ur\"(?:[-'\\u2018\\u2019][$\\w\\u00C0-\\u00FF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]+)*|[.]{3}\"\n",
      "    latin_scripts = latin1 + latin2\n",
      "    latin_punc = ur\"[!$%\\-&']\"\n",
      "    cjk_scripts = ur'[\\u4E00-\\u9FFF]|[\\u30A0-\\u30FF]+|[\\u3040-\\u309F]+|[\\uAC00-\\uD7AF]+'\n",
      "    cyr_scripts = ur\"[\\d\\u0400-\\u04FF]+(?:[-.'\\u2018\\u2019][\\d\\u0400-\\u04FF]+)*\"\n",
      "    cyr_special = ur\"(?:[\u0410-\u042f\\d&]+\\.)+\"\n",
      "    sa_scripts = ur'[\\u0900-\\u097F]+|[\\uA8E0-\\uA8FF]+|[\\u1CD0-\\u1CFF]+|[\\u0980-\\u09FF]+'\n",
      "    arb_scripts = ur'[\\u0600-\\u06FF\\u0750-\\u077F]+'\n",
      "    gre_scripts = ur'[\\u0370-\\u03FF\\u1F00-\\u1FFF]+'\n",
      "    all_scripts = '|'.join([special, date, song, latin_scripts, latin_punc, cjk_scripts, cyr_scripts, \n",
      "                            cyr_special, sa_scripts,  arb_scripts, gre_scripts])\n",
      "    pattern = re.compile(all_scripts)\n",
      "    with open(filepath, 'rb') as f:\n",
      "        for line in f:\n",
      "            record = line.decode('utf-8').strip('\\n')\n",
      "            entries = record.split(seperator)\n",
      "            for field, weight in zip(fields, weights):\n",
      "                record = entries[field]\n",
      "                token_list = re.split(splitor,record)\n",
      "                for token in token_list:\n",
      "                    subtoken_list = re.findall(pattern, token)\n",
      "                    for subtoken in subtoken_list:\n",
      "                        if subtoken.lower() in avoid_token_list:\n",
      "                            break\n",
      "                        subtoken = stem(subtoken)\n",
      "                        result[subtoken] = result.get(subtoken, 0) + weight\n",
      "    if normalize:\n",
      "        normalize_dict(result)\n",
      "        if logalize:\n",
      "            prob_to_log(result)\n",
      "    return result\n",
      "\n",
      "\n",
      "#training for song/music name using MusicBrainz data\n",
      "def train_song(filepath, seperator ,field, normalize=False,\n",
      "               logalize=False):\n",
      "    result = {}\n",
      "    stem_result = {}\n",
      "    avoid = ur'[][\":;~(){}/\uff1a\uff1b\uff5e\uff08\uff09\u300a\u300b\u3010\u3011\u3016\u3017\u300c\u300d\u300e\u300f\u201c\u201d\u00ab\u00bb\u263c\u301c\u25c4\u25ba]|[\\d]+(?:[-/.][\\d]+)+'\n",
      "    avoid_pat = re.compile(avoid)\n",
      "    avoid_token_list = set([u\"part\", u\"pt.\", u\"no.\", u\"op.\", u\"vs.\",u\"vs\",\n",
      "                        u\"feat\", u\"feat.\", u\"ft.\", u\"ft\",  u\"-\", u\"'\",\n",
      "                        u\"bwv\", u\"k.\"])\n",
      "    with open(filepath, 'rb') as f:\n",
      "        for line in f:\n",
      "            record = line.decode('utf-8').strip('\\n')\n",
      "            entries = record.split(seperator)\n",
      "            entry = entries[field]\n",
      "            if re.search(avoid_pat, entry):\n",
      "                continue\n",
      "            token_list = tokenize(entry)\n",
      "            avoid_entry = False\n",
      "            for token in token_list:\n",
      "                if token.lower() in avoid_token_list:\n",
      "                    avoid_entry = True\n",
      "                    break\n",
      "            if avoid_entry:\n",
      "                continue\n",
      "            for token in token_list:\n",
      "                if token:\n",
      "                    token = token.lower()\n",
      "                    result[token] = result.get(token, 0.0) + 1\n",
      "    for token in result:\n",
      "        freq = result[token]\n",
      "        token = stem(token)\n",
      "        stem_result[token] = stem_result.get(token, 0.0) + freq\n",
      "    del result\n",
      "    if normalize:\n",
      "        normalize_dict(stem_result)\n",
      "        if logalize:\n",
      "            prob_to_log(stem_result)\n",
      "    return stem_result\n",
      "\n",
      "\n",
      "#train for music description text using MusicBrainz data\n",
      "def train_other(insources, seperator,  normalize=False,\n",
      "                logalize=False):\n",
      "    result = {}\n",
      "    avoid = ur'[][\":;~(){}/\uff1a\uff1b\uff5e\uff08\uff09\u300a\u300b\u3010\u3011\u3016\u3017\u300c\u300d\u300e\u300f\u201c\u201d\u00ab\u00bb\u263c\u301c\u25c4\u25ba]|ft'\n",
      "    avoid_pat = re.compile(avoid)\n",
      "    for source in insources:\n",
      "        infile, field = source\n",
      "        with open(infile, 'rb') as f:\n",
      "            for line in f:\n",
      "                line = line.decode('utf-8').strip('\\n')\n",
      "                entries = line.split(seperator)\n",
      "                entry = entries[field]\n",
      "                for token in tokenize(entry):\n",
      "                    if re.search(avoid_pat, token):\n",
      "                        continue\n",
      "                    token = stem(token)\n",
      "                    result[token] = result.get(token, 0) + 1.0\n",
      "    if normalize:\n",
      "        normalize_dict(result)\n",
      "        if logalize:\n",
      "            prob_to_log(result)\n",
      "    return result\n",
      " \n",
      "\n",
      "#adjusting song name or artist name training dictionary\n",
      "def adjust(avoid, candidate, threshold = 10):\n",
      "    avoid_total = sum(avoid.values())\n",
      "    cand_total = sum(candidate.values())\n",
      "    cjk_scripts = ur'[\\u4E00-\\u9FFF]|[\\u30A0-\\u30FF]|[\\u3040-\\u309F]|[\\uAC00-\\uD7AF]'\n",
      "    cjk_pat = re.compile(cjk_scripts)\n",
      "    for key in avoid:\n",
      "        if re.search(cjk_pat, key):\n",
      "            continue\n",
      "        avoid_pct = avoid[key] / avoid_total\n",
      "        cand_pct = candidate.get(key, 0.0) / cand_total\n",
      "        if avoid_pct > cand_pct * threshold:\n",
      "            candidate.pop(key, None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "u'\u2013'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 152,
       "text": [
        "u'\\u2013'"
       ]
      }
     ],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "song_dict = train_song('mbdump/track', '\\t', 6, normalize=False, logalize=False)\n",
      "len(song_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 87,
       "text": [
        "1097904"
       ]
      }
     ],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(song_dict, open('song_dict.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 281
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "artist_dict = train_artist('mbdump/artist', [2,3], [1.0,1.0],'\\t', r'\\s+|,',\n",
      "                           normalize=False, logalize=False)\n",
      "len(artist_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 123,
       "text": [
        "399246"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(artist_dict, open('artist_dict.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 282
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#data scourse for music description dictionary\n",
      "source_list = [('mbdump/language', 4), ('mbdump/script', 3), ('mbdump/area', 2), \\\n",
      "               ('mbdump/area_alias', 2), ('mbdump/place', 2), ('mbdump/place_alias', 2), \\\n",
      "               ('mbdump/instrument', 2), ('mbdump/instrument_alias', 2), \\\n",
      "               ('mbdump/work_type', 1), ('mbdump/event', 2), ('mbdump/event_alias', 2), \\\n",
      "               ('mbdump/work_attribute_type_allowed_value', 2), ('mbdump/medium_format', 1)]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "other_dict = train_other(insources=source_list, seperator='\\t')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(other_dict, open('other_dict.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 283
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Consider to remove words from artist and song name dictionary \n",
      "#if found in following work type dictionary\n",
      "avoid_list = [('mbdump/instrument', 2), ('mbdump/instrument_alias', 2), \\\n",
      "              ('mbdump/work_attribute_type_allowed_value', 2)]\n",
      "avoid_dict = train_other(insources=avoid_list, seperator='\\t')\n",
      "len(avoid_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 119,
       "text": [
        "3954"
       ]
      }
     ],
     "prompt_number": 119
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Remove words from artist Song name dictionary if found in following work type\n",
      "#dictionary\n",
      "work_type = [('mbdump/work_type', 1), ('mbdump/medium_format', 1)]\n",
      "type_dict = train_other(insources=work_type, seperator='\\t')\n",
      "len(type_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 118,
       "text": [
        "89"
       ]
      }
     ],
     "prompt_number": 118
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#adjust(avoid=type_dict, candidate=emit_p['ARTIST'], threshold=0.00000001)\n",
      "adjust(avoid=type_dict, candidate=song_dict, threshold=0.00000001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 127
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "adjust(avoid=avoid_dict, candidate=artist_dict)\n",
      "adjust(avoid=avoid_dict, candidate=song_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Obtaining stopping works from 11 languages\n",
      "from nltk.corpus import stopwords\n",
      "stop_set = set()\n",
      "for language in stopwords.fileids():\n",
      "    for word in stopwords.words(language):\n",
      "        stop_set.add(normal_token(word))\n",
      "stop_set.add(u\"'\")\n",
      "stop_set.add(u'-')\n",
      "stop_set.discard(u'Only')\n",
      "stop_set.discard(u'Sia')\n",
      "len(stop_set)\n",
      "pickle.dump(stop_set, open('stop_set.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('stopwords.txt', 'rb') as f:\n",
      "    for line in f:\n",
      "        line = line.decode('utf-8').strip()\n",
      "        stop_set.add(stem(line))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in xrange(11):\n",
      "    stop_set.add(str(i))\n",
      "pickle.dump(stop_set, open('stop_set.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stop_set = pickle.load(open('stop_set.p', 'rb'))\n",
      "import sqlite3\n",
      "def hypen_tokenize(text):\n",
      "    latin = ur\"[\\w\\u00C0-\\u00FF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF\\u0400-\\u04FF]+|[-]\"\n",
      "    pattern = re.compile(latin)\n",
      "    token_list = re.findall(pattern, text)\n",
      "    return token_list\n",
      "\n",
      "\n",
      "\n",
      "def get_jf_dict(fromchar, tochar):\n",
      "    result = {}\n",
      "    for i, char in enumerate(fromchar):\n",
      "        result[char] = tochar[i]\n",
      "    return result\n",
      "\n",
      "\n",
      "def get_file_length(filepath):\n",
      "    count = 0\n",
      "    with open(filepath, 'rb') as f:\n",
      "        for line in f:\n",
      "            count += 1\n",
      "    return count\n",
      "\n",
      "\n",
      "#not in use now\n",
      "def tag_video(filepath, tag_dict, outpath):\n",
      "    with open(filepath, 'rb') as f_read, \\\n",
      "         open(outpath, 'wb') as f_write:\n",
      "        for line in f_read:\n",
      "            record = line.decode('utf-8').strip('\\n')\n",
      "            entries = record.split('<SEP>')\n",
      "            video = entries[0]\n",
      "            views = entries[1]\n",
      "            topics = entries[4].split('<and>')\n",
      "            title = entries[5]\n",
      "            if len(topics) < 2:\n",
      "                continue\n",
      "            token_tags ={}\n",
      "            for topic in topics:\n",
      "                label, t_list = tag_dict.get(topic, ('other', set()))\n",
      "                for t in t_list:\n",
      "                    token_tags[t] = label\n",
      "            token_list = tokenize(title)\n",
      "            token_list = map(stem, token_list)\n",
      "            token_tag_list = []\n",
      "            for token in token_list:\n",
      "                tag = token_tags.get(token, u'other')\n",
      "                token_tag = u\"<tag>\".join([token, tag])\n",
      "                token_tag_list.append(token_tag)\n",
      "            title_tag = u\"<and>\".join(token_tag_list)\n",
      "            video_tag = u\"<SEP>\".join([video, views, title_tag, title])+u\"\\n\"\n",
      "            f_write.write(video_tag.encode(\"utf-8\"))\n",
      "    \n",
      "\n",
      "\n",
      "def write_dict(input_dict, filepath, reverse=True, seperator='<SEP>'):\n",
      "    input_list = [(input_dict.get(key, ''), key) for key in input_dict]\n",
      "    input_list.sort(reverse=reverse)\n",
      "    with open(filepath, 'wb') as f:\n",
      "        for item in input_list:\n",
      "            record = '<SEP>'.join([item[1], unicode(item[0])])+u'\\n'\n",
      "            f.write(record.encode('utf-8'))\n",
      " \n",
      "            \n",
      "#not in use\n",
      "def explore_punc(filepath, punc_dict):\n",
      "    result = {}\n",
      "    with open(filepath, 'rb') as f:\n",
      "        for line in f:\n",
      "            record = line.decode('utf-8').strip('\\n')\n",
      "            entries = record.split('<SEP>')\n",
      "            views = entries[1]\n",
      "            tokentag_list = entries[2].split('<and>')\n",
      "            start_punc = None\n",
      "            end_punc = None\n",
      "            tag_set = set()\n",
      "            for tokentag in tokentag_list:\n",
      "                twins = tokentag.split('<tag>')\n",
      "                if len(twins) != 2:\n",
      "                    continue\n",
      "                token = twins[0].lower()\n",
      "                tag = twins[1].lower()\n",
      "                if start_punc:\n",
      "                    if token != end_mark:\n",
      "                        tag_set.add(tag)\n",
      "                    else:\n",
      "                        tag_list = list(tag_set)\n",
      "                        tag_list.sort()\n",
      "                        tag_string = '<and>'.join(tag_list)\n",
      "                        cur_dict = result.get(start_punc, {})\n",
      "                        cur_dict[tag_string] = cur_dict.get(tag_string, 0)+1\n",
      "                        result[start_punc] = cur_dict\n",
      "                        status_punc = None\n",
      "                        end_punc = None\n",
      "                else:\n",
      "                    if token in punc_dict:\n",
      "                        start_punc = token\n",
      "                        end_mark = punc_dict.get(start_punc, None)\n",
      "    for punc in result:\n",
      "        temp_dict = result.get(punc, None)\n",
      "        if temp_dict:\n",
      "            normalize_dict(temp_dict)\n",
      "            result[punc] = temp_dict\n",
      "    return result\n",
      "                        \n",
      "\n",
      "def normalize_dict(input_dict):\n",
      "    total = sum(input_dict.values())*1.0\n",
      "    for key in input_dict:\n",
      "        input_dict[key] /= total\n",
      "\n",
      "        \n",
      "def prob_to_log(input_dict):\n",
      "    for key in input_dict:\n",
      "        value = input_dict[key]\n",
      "        input_dict[key] = log(value)\n",
      "\n",
      "\n",
      "def get_punc(inputpath, punc, outpath):\n",
      "    pp = re.compile(punc, re.IGNORECASE)\n",
      "    with open(inputpath, 'rb') as f_read, \\\n",
      "         open(outpath, 'wb') as f_write:\n",
      "        for line in f_read:\n",
      "            record = line.decode('utf-8').strip('\\n')\n",
      "            entries = record.split('<SEP>')\n",
      "            title = entries[3]\n",
      "            if re.search(pp, title):\n",
      "                f_write.write(line)\n",
      "                \n",
      "#testing function not in use                \n",
      "def read_summary(inputpath):\n",
      "    result = {}\n",
      "    with open(inputpath, 'rb') as f:\n",
      "        for line in f:\n",
      "            record = line.decode('utf-8').strip('\\n')\n",
      "            entries = record.split('<SEP>')\n",
      "            if len(entries) != 3:\n",
      "                continue\n",
      "            tag_list = entries[2].split('<and>')\n",
      "            if len(tag_list) == 0:\n",
      "                continue\n",
      "            for tag in tag_list:\n",
      "                result[tag] = result.get(tag, 0)+1\n",
      "    normalize_dict(result)\n",
      "    return result\n",
      "\n",
      "#function not in use any more\n",
      "def get_tag(entry, sep1='<and>', sep2='<tag>'):\n",
      "    feature = set(['feature', 'feat.', 'feat', 'ft', 'ft.',\n",
      "                   'featur', 'by'])\n",
      "    seperator = u'-:;/|~\uff5e'  \n",
      "    start_quat = u'\"\u300c\u300e\u300a\u201c\u00ab'+u\"'\"\n",
      "    end_quat = u'\"\u300d\u300f\u300b\u201d\u00bb'+u\"'\"\n",
      "    start_para = u'[({\uff08\u3010\u3016'\n",
      "    end_para = u'])}\uff09\u3011\u3017'\n",
      "    wait = None\n",
      "    stack = []\n",
      "    result = []\n",
      "    for i, tokentag in enumerate(entry.split(sep1)):\n",
      "        twins = tokentag.split(sep2)\n",
      "        token = twins[0].lower()\n",
      "        tag = twins[1].upper()\n",
      "        if token in seperator:\n",
      "            tag = u'SEP'\n",
      "        elif token in feature and tag == 'OTHER':\n",
      "            tag = u'FEAT'\n",
      "        elif token != wait and token in start_quat:\n",
      "            stack.append((i, token))\n",
      "            wait = punc_pair[token]\n",
      "        elif token == wait and token in end_quat:\n",
      "            ind, start_token = stack.pop()\n",
      "            result[ind] = (start_token, u'SQ')\n",
      "            tag = u'EQ'\n",
      "            if stack:\n",
      "                wait = punc_pair[stack[-1][1]]\n",
      "            else:\n",
      "                wait = None\n",
      "        elif token in start_para:\n",
      "            stack.append((i, token))\n",
      "            wait = punc_pair[token]\n",
      "        elif token == wait and token in end_para:\n",
      "            ind, start_token = stack.pop()\n",
      "            result[ind] = (start_token, u'SP')\n",
      "            tag = u'EP'\n",
      "            if stack:\n",
      "                wait = punc_pair[stack[-1][1]]\n",
      "            else:\n",
      "                wait = None\n",
      "        result.append((token, tag))\n",
      "    return result\n",
      "\n",
      "#use this function for tokenization list before applying\n",
      "#the Viterbi algorithm\n",
      "def punc_tag(entry):\n",
      "    seperator = u'-:~'  \n",
      "    start_quat = u'\"\u300c\u300e\u300a\u201c\u00ab'+u\"'\"\n",
      "    end_quat = u'\"\u300d\u300f\u300b\u201d\u00bb'+u\"'\"\n",
      "    start_para = u'[({\uff08\u3010\u3016\u263c\u301c\u25c4'\n",
      "    end_para = u'])}\uff09\u3011\u3017\u263c\u301c\u25ba'\n",
      "    wait = []\n",
      "    stack = []\n",
      "    result = []\n",
      "    for i, token in enumerate(entry):\n",
      "        token = stem(token)\n",
      "        tag = 'OTHER'\n",
      "        \"\"\"\n",
      "        if token in seperator:\n",
      "            tag = u'SEP'\n",
      "        \"\"\"\n",
      "        if token not in wait and token in start_quat:\n",
      "            stack.append((i, token))\n",
      "            wait.append(punc_pair[token])\n",
      "        elif token in wait and token in end_quat:\n",
      "            check = wait.pop()\n",
      "            ind, start_token = stack.pop()\n",
      "            while token != check:\n",
      "                check = wait.pop()\n",
      "                ind, start_token = stack.pop()\n",
      "            result[ind] = (start_token, u'SQ')\n",
      "            tag = u'EQ'\n",
      "        elif token not in wait and token in start_para:\n",
      "            stack.append((i, token))\n",
      "            wait.append(punc_pair[token])\n",
      "        elif token in wait and token in end_para:\n",
      "            check = wait.pop()\n",
      "            ind, start_token = stack.pop()\n",
      "            while token != check:\n",
      "                check = wait.pop()\n",
      "                ind, start_token = stack.pop()\n",
      "            result[ind] = (start_token, u'SP')\n",
      "            tag = u'EP'\n",
      "        result.append((token, tag))\n",
      "    return result\n",
      "\n",
      "           \n",
      "def file_to_dict(inputfile):\n",
      "    result=[]\n",
      "    with open(inputfile, 'rb') as f:\n",
      "        for line in f:\n",
      "            record = line.decode('utf-8').strip('\\n')\n",
      "            entries = record.split('<SEP>')\n",
      "            entry = entries[2]\n",
      "            result.append(get_tag(entry))\n",
      "    return result\n",
      " \n",
      "    \n",
      "#tag_ind is the index of the youtube video title column in the input\n",
      "#data set    \n",
      "def trans_sep(infile, tag_ind, normalize=True, logalize=False):\n",
      "    write_out = False\n",
      "    required = set(['SONG', 'ARTIST', 'OTHER', 'FEAT',\n",
      "                    'SONG-', 'ARTIST-', 'OTHER-', \n",
      "                    'SONG&', 'ARTIST&', 'OTHER&',\n",
      "                    'SEP','JOIN'])\n",
      "    tran_dict = {}\n",
      "    count = 1\n",
      "    with open(infile, 'rb') as f:\n",
      "        for line in f:\n",
      "            record = line.decode('utf-8').strip('\\n')\n",
      "            entries = record.split('<SEP>')\n",
      "            tokentag_list = entries[tag_ind].split('<and>')\n",
      "            if len(tokentag_list) < 2:\n",
      "                count += 1\n",
      "                continue\n",
      "            pre_tag = tokentag_list[0].split('<tag>')[1]\n",
      "            for tokentag in tokentag_list[1:]:\n",
      "                cur_tag = tokentag.split('<tag>')[1]\n",
      "                if pre_tag in required or cur_tag in required:\n",
      "                    if pre_tag not in set(['EP', 'EP-', 'EP&']): \n",
      "                       #cur_tag not in set(['SP', 'SP-', 'SP&']):\n",
      "                        if pre_tag in set(['FEAT', 'FEAT-', 'FEAT&']):\n",
      "                            cur_tag = 'ARTIST'\n",
      "                        if cur_tag in set(['SP', 'SP-', 'SP&']):\n",
      "                            cur_tag = u'END'\n",
      "                        temp = tran_dict.get(pre_tag, {})\n",
      "                        temp[cur_tag] = temp.get(cur_tag, 0) + 1.0\n",
      "                        tran_dict[pre_tag] = temp\n",
      "                        \n",
      "                \"\"\"\n",
      "                if pre_tag == 'ARTIST&' and cur_tag == 'OTHER':\n",
      "                    print count\n",
      "                \"\"\"\n",
      "                if cur_tag == 'SEP':\n",
      "                    cur_tag = pre_tag.rstrip('[-&]')+'-'\n",
      "                    if cur_tag in set(['SP-', 'SQ-', 'FEAT-']):\n",
      "                        cur_tag.rstrip('-')\n",
      "                elif cur_tag == 'JOIN':\n",
      "                    cur_tag = pre_tag.rstrip('[-&]')+'&'\n",
      "                    if cur_tag in set(['SP&', 'SQ&', 'FEAT&']):\n",
      "                        cur_tag.rstrip('&')\n",
      "                pre_tag = cur_tag\n",
      "            #if write_out == True:\n",
      "            #    f1.write(line)\n",
      "            if pre_tag in required:\n",
      "                temp = tran_dict.get(pre_tag, {})\n",
      "                temp[u'END'] = temp.get(u'END', 0) + 1.0\n",
      "            count += 1\n",
      "            write_out = False\n",
      "    tt = tran_dict['FEAT'].pop(u'END', None)\n",
      "    if normalize is True:\n",
      "        for tag in tran_dict:\n",
      "            value = tran_dict[tag]\n",
      "            normalize_dict(value)\n",
      "            if logalize is True:\n",
      "                prob_to_log(value)\n",
      "            tran_dict[tag] = value\n",
      "    for key in set(['SP-', 'SQ-', 'FEAT-',\n",
      "                    'SP&', 'SQ&', 'FEAT&']):\n",
      "        tran_dict[key] = tran_dict[key.rstrip('[-&]')]\n",
      "    if normalize and logalize:\n",
      "        tran_dict['SONG&']['JOIN'] = tran_dict['SONG&'].get('JOIN', -2)\n",
      "        tran_dict['ARTIST&']['JOIN'] = tran_dict['ARTIST&'].get('JOIN', -2)\n",
      "        tran_dict['OTHER&']['JOIN'] = tran_dict['OTHER&'].get('JOIN', -2)\n",
      "        tran_dict['EQ&'] = tran_dict.setdefault('EQ&', \n",
      "                           {'EQ': log(0.5), 'SONG': log(0.5)})\n",
      "    return tran_dict\n",
      "\n",
      "\n",
      "    \n",
      "#tag_ind is the index of youtube video title column in the \n",
      "def start(infile, tag_ind, normalize=True, logalize=False):\n",
      "    start_dict = {}\n",
      "    required = set(['SONG', 'ARTIST', 'OTHER', 'FEAT', 'SEP', 'JOIN'])\n",
      "    with open(infile, 'rb') as f:\n",
      "        for line in f:\n",
      "            record = line.decode('utf-8').strip('\\n')\n",
      "            entries = record.split('<SEP>')\n",
      "            tokentag_list = entries[tag_ind].split('<and>')\n",
      "            for tokentag in tokentag_list:\n",
      "                twins= tokentag.split('<tag>')\n",
      "                if len(twins) != 2:\n",
      "                    continue\n",
      "                tag = twins[1]\n",
      "                if tag in required:\n",
      "                    start_dict[tag] = start_dict.get(tag, 0) + 1.0\n",
      "    if normalize is True:\n",
      "        normalize_dict(start_dict)\n",
      "        if logalize is True:\n",
      "            prob_to_log(start_dict)\n",
      "    return start_dict\n",
      "\n",
      "\n",
      "#stag is the tag we explore for token distribution\n",
      "#possible values for stag is 'SONG', 'ARTIST', 'OTHER', 'FEAT' \n",
      "#and 'SEP', 'JOIN'\n",
      "#tag_ind is the index of youtube video title colun in the input\n",
      "#dataset\n",
      "def token_dist(infile, stag, tag_ind, basic=None, normalize=True,\n",
      "               logalize=False):\n",
      "    count = 1\n",
      "    if basic is None:\n",
      "        token_dict = {}\n",
      "    else:\n",
      "        token_dict = basic\n",
      "    temp = token_dict.pop(u'ft', None)\n",
      "    with open(infile, 'rb') as f:\n",
      "        for line in f:\n",
      "            record = line.decode('utf-8').strip('\\n')\n",
      "            entries = record.split('<SEP>')\n",
      "            tokentag_list = entries[tag_ind].split('<and>')\n",
      "            for tokentag in tokentag_list:\n",
      "                twins = tokentag.split('<tag>')\n",
      "                if len(twins) != 2:\n",
      "                    continue\n",
      "                token = twins[0]\n",
      "                tag = twins[1]\n",
      "                if token == u'ft':\n",
      "                    tag = 'FEAT'\n",
      "                if tag == stag:\n",
      "                   token_dict[token] = token_dict.get(token, 0) + 1.0\n",
      "    if normalize is True:\n",
      "        normalize_dict(token_dict)\n",
      "        if logalize is True:\n",
      "            prob_to_log(token_dict)\n",
      "    return token_dict   \n",
      "    \n",
      "\n",
      "def token_freq(infile, token_set, relative=False):\n",
      "    token_count = 0\n",
      "    total_count = 0\n",
      "    with open(infile, 'rb') as f:\n",
      "        for line in f:\n",
      "            record = line.decode('utf-8').strip('\\n')\n",
      "            entries = record.split('<SEP>')\n",
      "            tokentag_list = entries[2].split('<and>')\n",
      "            for tokentag in tokentag_list:\n",
      "                twins = tokentag.split('<tag>')\n",
      "                if len(twins) != 2:\n",
      "                    continue\n",
      "                token = twins[0]\n",
      "                tag = twins[1].upper()\n",
      "                if token in token_set:\n",
      "                    token_count += 1.0\n",
      "                total_count += 1.0\n",
      "    \n",
      "    if relative is True:\n",
      "        token_count /= total_count\n",
      "    return token_count\n",
      "\n",
      "\n",
      "def tag_dist(infile, token_set, normalize=True, logalize=False):\n",
      "    tag_dict = {}\n",
      "    with open(infile, 'rb') as f:\n",
      "        for line in f:\n",
      "            record = line.decode('utf-8').strip('\\n')\n",
      "            entries = record.split('<SEP>')\n",
      "            tokentag_list = entries[2].split('<and>')\n",
      "            for tokentag in tokentag_list:\n",
      "                twins = tokentag.split('<tag>')\n",
      "                if len(twins) != 2:\n",
      "                    continue\n",
      "                token = twins[0]\n",
      "                tag = twins[1].upper()\n",
      "                if token in token_set:\n",
      "                    tag_dict[tag] = tag_dict.get(tag, 0) + 1.0\n",
      "    if normalize is True:\n",
      "        normalize_dict(tag_dict)\n",
      "        if logalize is True:\n",
      "            prob_to_log(tag_dict)\n",
      "    return tag_dict\n",
      "\n",
      "\n",
      "#functions implements Viterbi Algorithm\n",
      "def viterbi(obs, details = False, stemmer=False):\n",
      "    \"\"\"\n",
      "    obs is a list of text tokens with punctuation tag [];\n",
      "    states is a k-list of all k possible states of Markov Process;\n",
      "    start_p is a k-dict of probabilities of all k possible states;\n",
      "    trans_p is a k*k-dict of all transition probabilities among\n",
      "    k possible states;\n",
      "    emit_p is a k-dict of m-dict of probabilities of observing all \n",
      "    m possible tokens in a given state\n",
      "    \n",
      "    \"\"\"\n",
      "    punc = set(['SP','SQ', 'EP', 'EQ', \n",
      "                'SP-','SQ-', 'EP-', 'EQ-',\n",
      "                'SP&', 'SQ&', 'EP&', 'EQ&'])\n",
      "    end_punc = set(['SP', 'SP-', 'SP&'])\n",
      "    web = ur\"(?:https?://|www.)[\\w\\u00C0-\\u00FF]+(?:[./?=-][\\w\\u00C0-\\u00FF]+)*[/]?\"\n",
      "    web_pat = re.compile(web)\n",
      "    new_obs = []\n",
      "    for token in obs:        \n",
      "        if len(token)>1 and '-' in token:\n",
      "            stem_token = stem(token)\n",
      "            if stem_token not in emit_p['ARTIST'] and \\\n",
      "               stem_token not in emit_p['SONG']:\n",
      "                if re.search(web_pat, stem_token):\n",
      "                    new_obs.append(token)\n",
      "                    continue\n",
      "                for sub_token in hypen_tokenize(token):\n",
      "                    new_obs.append(sub_token)\n",
      "                continue\n",
      "        new_obs.append(token)\n",
      "    tokentag_list = punc_tag(new_obs)\n",
      "    trans_p['EP'] = start_p\n",
      "    trans_p['EP-'] = start_p\n",
      "    trans_p['EP&'] = start_p\n",
      "    record = [{}]\n",
      "    path = {}\n",
      "    last_punc = 'EP'\n",
      "    result = []\n",
      "    for tokentag in tokentag_list:\n",
      "        token = tokentag[0]\n",
      "        tag = tokentag[1]\n",
      "            \n",
      "        if last_punc and tag in punc:\n",
      "            result.append(tag)\n",
      "            last_punc = tag\n",
      "        elif last_punc and tag not in punc:\n",
      "            for state in states:\n",
      "                prob = trans_p[last_punc].get(state, -100) + \\\n",
      "                       emit_p[state].get(token, -100)\n",
      "                if state == 'SEP':\n",
      "                    state = last_punc + '-'\n",
      "                elif state == 'JOIN':\n",
      "                    state = last_punc + '&'\n",
      "                path[state] = [state]\n",
      "                record[0][state] = prob\n",
      "            last_punc = False\n",
      "        elif last_punc is False and tag not in punc:\n",
      "            newpath = {}\n",
      "            record.append({})\n",
      "            for state in states[:-2]:                  \n",
      "                prob, label = max((emit_p[state].get(token, -100) +\n",
      "                                   record[-2][s] +\n",
      "                                   trans_p[s].get(state, -100), s) \n",
      "                                   for s in record[-2])\n",
      "                record[-1][state] = prob\n",
      "                newpath[state] =  [item for item in path[label]]  \n",
      "                newpath[state].append(state)\n",
      "            for state in record[-2]:\n",
      "                if '&' in state:\n",
      "                    continue\n",
      "                prob_sep = emit_p['SEP'].get(token, -100) + \\\n",
      "                           record[-2][state] + \\\n",
      "                           trans_p[state].get('SEP', -100)\n",
      "                \n",
      "                state_sep = state.rstrip('[&-]') + '-'\n",
      "                if prob_sep > record[-1].get(state_sep, -float('inf')):\n",
      "                    record[-1][state_sep] = prob_sep\n",
      "                    newpath[state_sep] = [item for item in path[state]]\n",
      "                    newpath[state_sep].append(state_sep)\n",
      "            for state in record[-2]:\n",
      "                if '-' in state:\n",
      "                    continue\n",
      "                prob_and = emit_p['JOIN'].get(token, -100) + \\\n",
      "                           record[-2][state] + \\\n",
      "                           trans_p[state].get('JOIN', -100)\n",
      "                state_and = state.rstrip('[&-]') + '&'\n",
      "                if prob_and > record[-1].get(state_and, -float('inf')):\n",
      "                    record[-1][state_and] = prob_and\n",
      "                    newpath[state_and] = [item for item in path[state]]\n",
      "                    newpath[state_and].append(state_and)\n",
      "            path = newpath\n",
      "        elif last_punc is False and tag in punc:\n",
      "            last_punc = tag\n",
      "            newtag = tag\n",
      "            if tag in end_punc:\n",
      "                newtag = u'END'\n",
      "            prob, label = max((record[-1][s] + \n",
      "                               trans_p[s].get(newtag, -100), s) \n",
      "                               for s in record[-1])\n",
      "            result += path[label]\n",
      "            result.append(tag)\n",
      "            record = [{}]\n",
      "            path = {}\n",
      "    if path:\n",
      "        prob, label = max((record[-1][s] +\n",
      "                           trans_p[s].get('END', -100), s) \n",
      "                           for s in record[-1])\n",
      "        result += path[label]\n",
      "    for i, tag in enumerate(result):\n",
      "        if '-' in tag:\n",
      "            result[i] = 'SEP'\n",
      "        elif '&' in tag:\n",
      "            result[i] = 'JOIN'\n",
      "    if details:\n",
      "        print prob\n",
      "        for item in record:\n",
      "            print item\n",
      "\n",
      "    if stemmer == True:\n",
      "        token_list = [item[0] for item in tokentag_list]\n",
      "        return zip(token_list, result)\n",
      "    return zip(new_obs, result)\n",
      "\n",
      "\n",
      "\n",
      "#Part of Speach Tagging of YouTube Video title\n",
      "#info_ind are the list of index of included columns from input \n",
      "#data set\n",
      "#title_id are the index of youtube video title from input data \n",
      "def tag_title(inpath, outpath, info_ind, title_ind, stemmer=True):\n",
      "    states = ['SONG', 'ARTIST', 'OTHER', 'FEAT', 'SEP', 'JOIN']\n",
      "    with open(inpath, 'rb') as f_read, \\\n",
      "         open(outpath, 'wb') as f_write:\n",
      "        for line in f_read:\n",
      "            record = line.decode('utf-8').strip()\n",
      "            entries = record.split('<SEP>')\n",
      "            infos = [entries[ind] for ind in info_ind]\n",
      "            entry = entries[title_ind]\n",
      "            if entry == 'None' or len(entry) == 0:\n",
      "                continue\n",
      "            temp = tokenize(entry)\n",
      "            tuple_list = viterbi(temp, stemmer=stemmer)\n",
      "            tokentag_list = [u'<tag>'.join(item)  for item in tuple_list]\n",
      "            tagging = u'<and>'.join(tokentag_list)\n",
      "            infos.append(tagging)\n",
      "            result = u'<SEP>'.join(infos)+'\\n'\n",
      "            f_write.write(result.encode('utf-8'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trend1825 = pickle.load(open('trend0218-0225.p', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle as pickle\n",
      "from math import exp\n",
      "coef = pickle.load(open('/home/galaxy/Documents/Classification/coef.p', 'rb'))\n",
      "token_dict = pickle.load(open('/home/galaxy/Documents/Classification/token_dict.p', 'rb'))\n",
      "import sqlite3\n",
      "from scipy.sparse import coo_matrix, csr_matrix\n",
      "#functions that compute the YouTube video trend\n",
      "#infile can be something like video0225.txt etc.\n",
      "def trend(infile1, infile2, field1=1, field2=1,\n",
      "          write=True):\n",
      "    view_diff = {}\n",
      "    previous = {}\n",
      "    current = {}\n",
      "    with open(infile1, 'rb') as f1:\n",
      "        for line in f1:\n",
      "            record = line.decode('utf-8').strip()\n",
      "            entries = record.split('<SEP>')\n",
      "            video = entries[0]\n",
      "            try:\n",
      "                entry = int(entries[field1])\n",
      "            except:\n",
      "                continue\n",
      "            previous[video] = entry\n",
      "    with open(infile2, 'rb') as f2:\n",
      "        for line in f2:\n",
      "            record = line.decode('utf-8').strip()\n",
      "            entries = record.split('<SEP>')\n",
      "            video = entries[0]\n",
      "            try:\n",
      "                entry = int(entries[field2])\n",
      "            except:\n",
      "                continue\n",
      "            current[video] = entry\n",
      "    for key in previous:\n",
      "        if key in current:\n",
      "            view_diff[key] = current[key] - previous[key]\n",
      "    if write:\n",
      "        time1 = infile1.rstrip('.txt')[-4:]\n",
      "        time2 = infile2.rstrip('.txt')[-4:]\n",
      "        filename = 'trend'+'-'.join([time1, time2])+'.p'\n",
      "        pickle.dump(view_diff, open(filename, 'wb'))\n",
      "    return view_diff\n",
      "\n",
      "\n",
      "#adding language term for a tag SONG or ARTIST\n",
      "def lang_tag(token, tag):\n",
      "    cj_scripts = ur'[\\u4E00-\\u9FFF]|[\\u30A0-\\u30FF]|[\\u3040-\\u309F]'\n",
      "    kr_scripts = ur'[\\uAC00-\\uD7AF]'\n",
      "    \n",
      "    cj_pat = re.compile(cj_scripts)\n",
      "    kr_pat = re.compile(kr_scripts)\n",
      "    \n",
      "    if re.search(cj_pat, token):\n",
      "        tag += u'_CJ'\n",
      "    elif re.search(kr_pat, token):\n",
      "        tag += u'_KR'\n",
      "    else:\n",
      "        tag += u'_LA'\n",
      "    return tag\n",
      "\n",
      "\n",
      "#standardize token\n",
      "def normal_token(token):\n",
      "    special = ur\"(?:[\\w\\u00C0-\\u00FF\\u0400-\\u04FF][/.])+[\\w\\u00C0-\\u00FF\\u0400-\\u04FF]?\"\n",
      "    ch_scripts = ur'[\\u4E00-\\u9FFF]' \n",
      "    special_pat = re.compile(special)\n",
      "    ch_pat = re.compile(ch_scripts)\n",
      "    if re.search(ch_pat, token):\n",
      "        return ch_stem(token)\n",
      "    elif re.search(special_pat, token):\n",
      "        return token.upper()\n",
      "    elif u'-' in token:\n",
      "        subtoken_list = hypen_tokenize(token)\n",
      "        new_list = [subtoken.capitalize() \\\n",
      "                   for subtoken in subtoken_list]\n",
      "        return ''.join(new_list)\n",
      "    else:\n",
      "        return token.capitalize()\n",
      "            \n",
      "\n",
      "#process tokentag_list \n",
      "#Combine Two songs with JOIN tag as one song\n",
      "def combine_song(tokentag_list):\n",
      "    previous_token = None\n",
      "    previous_tag = u'START'\n",
      "    total_join = 0\n",
      "    correction = []\n",
      "    for i, tokentag in enumerate(tokentag_list):\n",
      "        token = tokentag[0]\n",
      "        tag = tokentag[1]\n",
      "        if previous_tag == 'SONG' and tag == 'JOIN':\n",
      "            if token == u'&':\n",
      "                total_join += 1\n",
      "                correction.append(i)\n",
      "        elif previous_tag == 'SONG' and tag != 'SONG':\n",
      "            if total_join == 1:\n",
      "                total_join -= 1\n",
      "            elif total_join > 1:\n",
      "                while total_join > 0:\n",
      "                    correction.pop()\n",
      "                    total_join -= 1                \n",
      "        elif previous_tag == 'JOIN' and tag != 'SONG':\n",
      "            if len(correction) == 0:\n",
      "                continue\n",
      "            if correction[-1] == i - 1:\n",
      "                total_join -= 1\n",
      "                temp = correction.pop()\n",
      "        previous_token = token\n",
      "        previous_tag = tag\n",
      "    if correction:\n",
      "        if i == correction[-1]:\n",
      "            while total_join > 0:\n",
      "                correction.pop()\n",
      "                total_join -= 1\n",
      "        for ind in correction:\n",
      "            tokentag_list[ind] = (u'&', 'SONG')\n",
      "                \n",
      "\n",
      "#decide if a single token is stopping words\n",
      "def is_stop(token):\n",
      "    if token in stop_set:\n",
      "        return True\n",
      "    try:\n",
      "        token = int(token)\n",
      "        if token != 22:\n",
      "            return True\n",
      "    except:\n",
      "        return False\n",
      "\n",
      "  \n",
      "#predictive model to filter out non-music model\n",
      "def get_feature(text, simple_token=False):\n",
      "    dim = len(token_dict)\n",
      "    row = []\n",
      "    col = []\n",
      "    data = []\n",
      "    token_list, obs = stem_tokenize(text, simple_token)   \n",
      "    for token in token_list:\n",
      "        ind = token_dict.get(token, None)\n",
      "        if ind:\n",
      "            row.append(0)\n",
      "            col.append(ind)\n",
      "            data.append(1.0)\n",
      "    feature = coo_matrix((data, (row, col)), \n",
      "                         shape=(1, dim)).tocsr()\n",
      "    if simple_token:\n",
      "        return feature, obs\n",
      "    return feature\n",
      "\n",
      "def classify(token_list):\n",
      "    xbeta = 0\n",
      "    prob = 0\n",
      "    for token in token_list:\n",
      "        ind = token_dict.get(token, None)\n",
      "        if ind:\n",
      "            xbeta += coef[ind]\n",
      "    xbeta += coef[-1]\n",
      "    prob = 1 / (1 + exp(-xbeta))\n",
      "    if prob >= 0.5:\n",
      "        return 1\n",
      "    else:\n",
      "        return 0\n",
      "    \n",
      "\n",
      "def stem_tokenize(text, simple_token=False):\n",
      "    web = ur\"(?:https?://|www.)[\\w\\u00C0-\\u00FF]+(?:[./?=-][\\w\\u00C0-\\u00FF]+)*[/]?\"\n",
      "    web_pat = re.compile(web)\n",
      "    new_obs = []\n",
      "    body = text.split('<newline>')\n",
      "    title = body[0]\n",
      "    obs = tokenize(title)\n",
      "    for item in obs:\n",
      "        if u'-' in item and len(item) > 6:\n",
      "            temp = hypen_tokenize(item)\n",
      "            for token in temp:\n",
      "                new_obs.append(stem(token))\n",
      "        else:\n",
      "            new_obs.append(stem(item))\n",
      "    for sentence in body[1:]:\n",
      "        if not sentence:\n",
      "            continue\n",
      "        obs = tokenize(sentence, 'nopunc')\n",
      "        for item in obs:\n",
      "            if re.search(web_pat, item):\n",
      "                continue\n",
      "            if u'-' in item and len(item) > 6:\n",
      "                temp = hypen_tokenize(item)\n",
      "                for token in temp:\n",
      "                    new_obs.append(stem(token))\n",
      "            else:\n",
      "                new_obs.append(stem(item))\n",
      "    if simple_token:\n",
      "        return new_obs, obs\n",
      "    return new_obs\n",
      "    \n",
      "    \n",
      "#get artist entity list and song entity list for a youtube video\n",
      "#source variable can be either\n",
      "def entity(entry, source='raw', check=False):\n",
      "    tt = None\n",
      "    if check:\n",
      "        new_obs, tt = stem_tokenize(entry, True)\n",
      "        if classify(new_obs) == 0:\n",
      "            return 'no music'\n",
      "    avoid_token = set([',', '\uff0c', '.', '\u3002'])\n",
      "    required = set(['ARTIST', 'SONG'])\n",
      "    result = {'ARTIST':set(), 'SONG':set()}  \n",
      "    if entry == u'None':\n",
      "        return result\n",
      "    if source == 'intermediate':\n",
      "        temp_list = entry.split('<and>')\n",
      "        tokentag_list = [item.split('<tag>') \\\n",
      "                         for item in temp_list \\\n",
      "                         if '<tag>' in item]\n",
      "    elif source == 'raw':\n",
      "        if not tt:\n",
      "            tt = tokenize(entry)\n",
      "        tokentag_list = viterbi(tt, stemmer=False)\n",
      "    combine_song(tokentag_list)\n",
      "    tokentag_list.append((u'None', u'END'))\n",
      "    current_token = normal_token(tokentag_list[0][0])\n",
      "    current_tag = tokentag_list[0][1].upper()\n",
      "    current_tag = lang_tag(current_token, current_tag)\n",
      "    temp_entity = []\n",
      "    if current_tag[:-3] in required:\n",
      "        temp_entity.append(current_token)\n",
      "    for tokentag in tokentag_list[1:]:\n",
      "        previous_token = current_token\n",
      "        previous_tag = current_tag\n",
      "        current_token = normal_token(tokentag[0])\n",
      "        current_tag = tokentag[1].upper()\n",
      "        current_tag = lang_tag(current_token, current_tag)\n",
      "        if temp_entity and current_tag != previous_tag:\n",
      "            while temp_entity[-1] in avoid_token:\n",
      "                temp_entity.pop()\n",
      "                if not temp_entity:\n",
      "                    break\n",
      "            if len(temp_entity) == 1:\n",
      "                temp = temp_entity[0]\n",
      "                if is_stop(temp):\n",
      "                    temp_entity = []\n",
      "            if previous_tag[-3:] == u'_CJ':\n",
      "                entity_string = ''.join(temp_entity)\n",
      "            else:\n",
      "                entity_string = ' '.join(temp_entity)\n",
      "            if entity_string:\n",
      "                result[previous_tag[:-3]].add(entity_string)\n",
      "            temp_entity = []                    \n",
      "        if current_tag[:-3] in required:\n",
      "            temp_entity.append(current_token)\n",
      "    return result\n",
      "\n",
      "\n",
      "#combination of elements from two list\n",
      "def combination(set1, set2):\n",
      "    if not set1:\n",
      "        set1.add(u'None')\n",
      "    if not set2:\n",
      "        set2.add(u'None')\n",
      "    result = []\n",
      "    for item1 in set1:\n",
      "        for item2 in set2:\n",
      "            result.append((item1, item2))\n",
      "    return result\n",
      "\n",
      "\n",
      "#construct data set of video artist, song name infomation\n",
      "#in the output data set, there are 7 columns\n",
      "#Col0. YouTube video ID\n",
      "#Col1. YouTube vide views increase in the last week\n",
      "#Col2. Artist  name of this YouTube Video\n",
      "#Col3, Song name of this YouTube Video\n",
      "#Col4, Number of artists in this YouTube Video\n",
      "#Col5, Number of songs in this YouTube Video\n",
      "#Col6, YouTube video Title\n",
      "def name_title(trend_dict, infile, outfile, title_ind=4,\n",
      "               source='raw', check=False):\n",
      "    with open(infile, 'rb') as f_read, \\\n",
      "         open(outfile, 'wb') as f_write:\n",
      "        record = []\n",
      "        for line in f_read:\n",
      "            line = line.decode('utf-8').strip()\n",
      "            entries = line.split('<SEP>')\n",
      "            video = entries[0]\n",
      "            if video not in trend_dict:\n",
      "                continue            \n",
      "            entry = entries[title_ind]\n",
      "            view_trend = unicode(trend_dict[video])\n",
      "            meta = entity(entry, source=source, check=check)\n",
      "            if meta == 'no music':\n",
      "                continue\n",
      "            pairs = combination(meta['ARTIST'], meta['SONG'])\n",
      "            artist_count =unicode(len(meta['ARTIST']))\n",
      "            song_count = unicode(len(meta['SONG']))\n",
      "            for pair in pairs:\n",
      "                obs_list = [video, view_trend, pair[0], pair[1], \\\n",
      "                            artist_count, song_count, entry]\n",
      "                record.append(u'<SEP>'.join(obs_list)) \n",
      "            record_stack = u'\\n'.join(record) + u'\\n'\n",
      "            f_write.write(record_stack.encode('utf-8'))\n",
      "            record = []\n",
      "                \n",
      "\n",
      "#Obtain entity trend of YouTube Video titles\n",
      "#a entity can be an artist name, a song name, or\n",
      "#an artist-song name pair\n",
      "#the infile argument should be output of name_title function\n",
      "def entity_trend(infile, entity_fields, rep_field, \n",
      "                 include_id=False, view_field=1):\n",
      "    if len(entity_fields) > 1:\n",
      "        rep_field = None\n",
      "    result = {}\n",
      "    with open(infile, 'rb') as f:\n",
      "        for line in f:\n",
      "            line = line.decode('utf-8').strip()\n",
      "            entries = line.split('<SEP>')\n",
      "            entity_list = [entries[field] \\\n",
      "                           for field in entity_fields]\n",
      "            entity_string = u'<SEP>'.join(entity_list)\n",
      "            view = float(entries[view_field])\n",
      "            if include_id:\n",
      "                video = entries[0]\n",
      "                title = entries[6]\n",
      "            try:\n",
      "                rep = float(entries[rep_field])\n",
      "            except:\n",
      "                rep = 1.0\n",
      "            weight = view / rep\n",
      "            '''\n",
      "            values on result dictionary is a list of length 3\n",
      "            values[0] is total trend last week\n",
      "            values[1] is the id of the representative youtube\n",
      "            video id\n",
      "            values[2] is the trend of the represetative\n",
      "            youtube video\n",
      "            \n",
      "            '''\n",
      "            if include_id:\n",
      "                temp = result.get(entity_string, [0, u'None', -float('inf')])\n",
      "                if view >= temp[2]:\n",
      "                    temp[1] = video\n",
      "                    temp[2] = view\n",
      "            else:\n",
      "                temp = result.get(entity_string, [0])\n",
      "            temp[0] += weight\n",
      "            result[entity_string] = temp\n",
      "    return result\n",
      "\n",
      "\n",
      "#Manipulate SQLite Database using external SQL scripts\n",
      "def control_db(dbfile, scripts):\n",
      "    conn = sqlite3.connect(dbfile)\n",
      "    c = conn.cursor()\n",
      "    with open(scripts, 'rb') as f:\n",
      "        rr = f.read()\n",
      "    try:\n",
      "        c.executescript(rr)\n",
      "    except:\n",
      "        conn.close()\n",
      "    conn.commit()\n",
      "    conn.close()\n",
      "\n",
      " \n",
      "#Create a customized generator for reading files\n",
      "#infile is output of name_title function\n",
      "def file_generator(infile, fields, int_ind, seperator=u'<SEP>'):\n",
      "    with open(infile, 'rb') as f:\n",
      "        for line in f:\n",
      "            line = line.decode('utf-8').strip()\n",
      "            entries = line.split(seperator)\n",
      "            result = []\n",
      "            for i, entry in enumerate(entries):\n",
      "                if i in fields:\n",
      "                    if i in int_ind:\n",
      "                        try:\n",
      "                            entry = int(entry)\n",
      "                        except:\n",
      "                            pass\n",
      "                    if entry == u'None':\n",
      "                        entry = None\n",
      "                    result.append(entry)\n",
      "            yield tuple(result)\n",
      "    \n",
      "    \n",
      "#Transform a file into a SQL database table\n",
      "#infile is output of name_title function\n",
      "def file_to_db(infile, dbfile, table, fields=(0,1,2,3,6), int_ind=(1,),\n",
      "               seperator=u'<SEP>'):\n",
      "    conn = sqlite3.connect(dbfile)\n",
      "    c = conn.cursor()\n",
      "    arguments = file_generator(infile=infile, fields=fields,\n",
      "                               int_ind = int_ind, \n",
      "                               seperator=seperator)\n",
      "    placeholder = '('+','.join(['?' for i in range(len(fields))])+')'\n",
      "    script = ' '.join(['INSERT INTO', table, 'VALUES', placeholder])\n",
      "    try:\n",
      "        c.executemany(script, arguments)\n",
      "    except:\n",
      "        conn.close()\n",
      "    conn.commit()\n",
      "    conn.close()\n",
      "    \n",
      "    \n",
      "#create a customized generator for reading dictionary\n",
      "#in_dict arguments are output from entity_trend function\n",
      "def dict_generator(in_dict, key_fields, key_int, value_fields, \n",
      "                   value_int, seperator='<SEP>'):\n",
      "    for key in in_dict:\n",
      "        entries = key.split(seperator)\n",
      "        result = []\n",
      "        for i, entry in enumerate(entries):\n",
      "            if i in key_fields:\n",
      "                if i in key_int:\n",
      "                    try:\n",
      "                        entry = int(entry)\n",
      "                    except:\n",
      "                        pass\n",
      "                if entry == u'None':\n",
      "                    entry = None\n",
      "                result.append(entry)\n",
      "        for j, value in enumerate(in_dict[key]):\n",
      "            if j in value_fields:\n",
      "                if j in value_int:\n",
      "                    try:\n",
      "                        value = int(value)\n",
      "                    except:\n",
      "                        pass\n",
      "                if value == u'None':\n",
      "                    value = None\n",
      "                result.append(value)\n",
      "        yield result\n",
      "        \n",
      "        \n",
      "#Transform a dictionary into a SQL database table\n",
      "def dict_to_db(in_dict, dbfile, table, key_fields=(0,), \n",
      "               key_int=(), value_fields=(0,), \n",
      "               value_int=(0,), seperator=u'<SEP>'):\n",
      "    conn = sqlite3.connect(dbfile)\n",
      "    c = conn.cursor()\n",
      "    arguments = dict_generator(in_dict=in_dict, \n",
      "                               key_fields=key_fields,\n",
      "                               key_int=key_int, \n",
      "                               value_fields=value_fields,\n",
      "                               value_int=value_int,\n",
      "                               seperator=seperator)\n",
      "    arg_length = len(key_fields) + len(value_fields)\n",
      "    placeholder = '(' + ','.join(['?' for i in xrange(arg_length)]) + ')'\n",
      "    script = ' '.join(['INSERT INTO', table, 'VALUES', placeholder])\n",
      "    try:\n",
      "        c.executemany(script, arguments)\n",
      "    except:\n",
      "        conn.close()\n",
      "    conn.commit()\n",
      "    conn.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "coef = list(model.coef_[0])\n",
      "coef.append(model.intercept_[0])\n",
      "pickle.dump(coef, open('coef.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = pickle.load(open('/home/galaxy/Documents/Classification/model.p', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit classify(ff)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10000 loops, best of 3: 103 \u00b5s per loop\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ff, ttt = get_feature(u'Mathematics: The Next Generation - Professor Peter Cameron', True)\n",
      "model.predict_proba(X=ff), model.predict(X=ff)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 60,
       "text": [
        "(array([[ 0.7102624,  0.2897376]]), array([0]))"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "entity(u'Selena Gomez - The Heart Wants What It Wants (Official Video)',\\\n",
      "       source='raw', check=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "{'ARTIST': {u'Selena Gomez'}, 'SONG': {u'The Heart Wants What It Wants'}}"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#functions that compute the YouTube video trend\n",
      "#infile can be something like video0225.txt etc.\n",
      "trend2803 = trend('video0728.txt', 'video0803.txt', field1=1, field2=1,\n",
      "          write=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "name_title(trend_dict=trend2803, infile='video0803.txt',\n",
      "           outfile='video_entity0803.txt', title_ind=4,\n",
      "           source = 'raw', check=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_file_length('video_entity0531.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "2610949"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "control_db('youtube.db', 'initial.sql')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "control_db('youtube.db', 'index.sql')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_to_db(infile='video_entity0803.txt', dbfile='youtube.db', table='meta')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "artist_trend = entity_trend(infile='video_entity0803.txt', entity_fields=[2], \n",
      "                            rep_field=5)\n",
      "len(artist_trend)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "451856"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "artist_trend = pickle.load(open('artist_trend.p', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(artist_trend, open('artist_trend.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dict_to_db(in_dict=artist_trend, dbfile='youtube.db', table='artist_trend')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "song_trend = entity_trend(infile='video_entity0803.txt', entity_fields=[3],\n",
      "                          rep_field=4, include_id=False)\n",
      "len(song_trend)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "816928"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "song_trend = pickle.load(open('song_trend.p', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(song_trend, open('song_trend.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dict_to_db(in_dict=song_trend, dbfile='youtube.db', table='song_trend',\n",
      "           key_fields=(0,), key_int=(), value_fields=(0,), value_int=(0,))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "track_trend = entity_trend(infile='video_entity0803.txt', entity_fields=[2,3],\n",
      "                           rep_field=None, include_id=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(track_trend)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "2039551"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count = 0\n",
      "rr = []\n",
      "for item in track_trend.values():\n",
      "    if item[1] == u'None':\n",
      "        count += 1\n",
      "        rr = item\n",
      "count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "1 < -float('inf')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "False"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "control_db('youtube.db', 'index.sql')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(track_trend)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "1771917"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(track_trend, open('track_trend.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "track_trend = pickle.load(open('track_trend.p', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dict_to_db(in_dict=track_trend, dbfile='youtube.db', table='track_trend',\n",
      "           key_fields=(0,1), key_int=(), value_fields=(0,1), value_int=(0,))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'haha'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "haha\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "name_title(trend_dict=trend1825, infile='video0225.txt',\n",
      "           outfile='video_entity0225.txt', title_ind=4,\n",
      "           source = 'raw')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Getting 6 column title entity data set. 6 columns"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = u\"Aliz\u00e9e - Gourmandises [2001]\"\n",
      "rr = entity(entry=test, source='raw')\n",
      "print 'ARTIST:'\n",
      "for aa in rr['ARTIST']:\n",
      "    print aa\n",
      "print '\\n'\n",
      "print 'SONG:'\n",
      "for ss in rr['SONG']:\n",
      "    print ss"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ARTIST:\n",
        "Aliz\u00e9e\n",
        "\n",
        "\n",
        "SONG:\n",
        "Gourmandises\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit entity(entry=test, source='raw')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1000 loops, best of 3: 1.25 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trend1825 = trend(infile1='video0218.txt', infile2='video0225.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Getting trending information between Feb. 11 and Feb. 18"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_punc('retrain5_tag0-42.txt', u'[-:]', u'result_dash.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count = 1\n",
      "ind = 1\n",
      "with open('video_tag0211.txt', 'rb') as f, open('video_tag50000.txt', 'wb') as f1:\n",
      "    for line in f:\n",
      "        if ind % 103 == 0:\n",
      "            f1.write(line)\n",
      "            count += 1\n",
      "        ind += 1\n",
      "        if count > 50000:\n",
      "            break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 268
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tran_prob = {}\n",
      "for key in ['ARTIST-', 'SONG-', 'OTHER-', 'ARTIST', 'SONG', 'OTHER']:\n",
      "    tran_prob[key] = aa_all[key]\n",
      "for key in ['EP', 'EQ', 'EP-', 'EQ-']:\n",
      "    tran_prob[key] = aa_qtotal[key]\n",
      "for key in ['FEAT']:\n",
      "    tran_prob[key] = aa_ftotal[key]\n",
      "for key in ['SP', 'SQ']:\n",
      "    tran_prob[key] = aa_total[key]\n",
      "for key in ['FEAT-', 'SP-', 'SQ-']:\n",
      "    tran_prob[key] = tran_prob[key.rstrip('-')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 193
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p = pickle.load(open('emit_p.p', 'rb'))\n",
      "tran_p = pickle.load(open('tran_p.p', 'rb'))\n",
      "start_p = pickle.load(open('start_p.p', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "write_dict(ss_dict, 'ss_dict.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tran_p['SP&'] = tran_p['SP']\n",
      "tran_p['SQ&'] = tran_p['SQ']\n",
      "tran_p['EQ&'] = {'EQ': -0.5, 'SONG': -0.5}\n",
      "start_p = start('video_tag500.txt', normalize=True, logalize=True)\n",
      "tran_p =trans_sep('video_tag650.txt', normalize=True, logalize=True)\n",
      "tran_p['SONG-']['SEP'] = tran_p['ARTIST-']['SEP']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trans_p =trans_sep(infile='video_tag0211.txt', tag_ind=3,\n",
      "                   normalize=True, logalize=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 347
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_p = start(infile='video_tag0211.txt', tag_ind=3,\n",
      "                normalize=True, logalize=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 414
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trans_p['SONG']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 349,
       "text": [
        "{u'ARTIST': -6.378759508193729,\n",
        " u'END': -1.7105807094089869,\n",
        " u'EP': -5.016855137718138,\n",
        " u'EQ': -3.7891559442321783,\n",
        " u'FEAT': -4.977229265030084,\n",
        " u'JOIN': -5.124129984302715,\n",
        " u'OTHER': -2.824411079713678,\n",
        " u'SEP': -2.935512682451613,\n",
        " u'SONG': -0.4143088665321103,\n",
        " u'SQ': -6.113163752828455}"
       ]
      }
     ],
     "prompt_number": 349
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trans_p['SONG-']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 350,
       "text": [
        "{u'ARTIST': -0.40488625488754554,\n",
        " u'END': -3.6373877426204033,\n",
        " u'FEAT': -5.270393661138583,\n",
        " u'OTHER': -1.2754006522066093,\n",
        " u'SEP': -3.8092098689648983}"
       ]
      }
     ],
     "prompt_number": 350
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trans_p['ARTIST']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 351,
       "text": [
        "{u'ARTIST': -0.8479104385023443,\n",
        " u'END': -2.522700262319229,\n",
        " u'EP': -3.3838562864251833,\n",
        " u'EQ': -7.875150637447193,\n",
        " u'FEAT': -4.292378505559481,\n",
        " u'JOIN': -2.7758102625987484,\n",
        " u'OTHER': -2.0507521068917436,\n",
        " u'SEP': -1.4514441609994122,\n",
        " u'SONG': -4.756758351157314,\n",
        " u'SQ': -4.633853069144262}"
       ]
      }
     ],
     "prompt_number": 351
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trans_p['ARTIST']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 416,
       "text": [
        "{u'ARTIST': -0.8570335415285959,\n",
        " u'END': -2.4991653752182375,\n",
        " u'EP': -3.3214558165338754,\n",
        " u'EQ': -7.831757182890529,\n",
        " u'FEAT': -4.301801330076928,\n",
        " u'JOIN': -2.75923023670162,\n",
        " u'OTHER': -2.0538774034868794,\n",
        " u'SEP': -1.4533806969198455,\n",
        " u'SONG': -4.779273122695593,\n",
        " u'SQ': -4.6430819432234385}"
       ]
      }
     ],
     "prompt_number": 416
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trans_p['ARTIST-']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 183,
       "text": [
        "{u'ARTIST': -13.118608336052832,\n",
        " u'END': -5.334343195208385,\n",
        " u'EQ': -8.286302577480994,\n",
        " u'OTHER': -2.9693746069930653,\n",
        " u'SEP': -4.643487921058503,\n",
        " u'SONG': -0.10901284123475202,\n",
        " u'SQ': -3.2902485062085436}"
       ]
      }
     ],
     "prompt_number": 183
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trans_p['OTHER']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 352,
       "text": [
        "{u'ARTIST': -2.5062628753878933,\n",
        " u'END': -1.7468021845679789,\n",
        " u'EP': -1.827556791004915,\n",
        " u'EQ': -6.329215767622167,\n",
        " u'FEAT': -4.814100089942673,\n",
        " u'JOIN': -4.02581982724823,\n",
        " u'OTHER': -0.8777370729158162,\n",
        " u'SEP': -2.1566720976394183,\n",
        " u'SONG': -4.027036345003972,\n",
        " u'SQ': -5.069587857015405}"
       ]
      }
     ],
     "prompt_number": 352
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trans_p['OTHER-']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 353,
       "text": [
        "{u'ARTIST': -1.76223408764185,\n",
        " u'END': -4.034421225060335,\n",
        " u'OTHER': -1.388926381260299,\n",
        " u'SEP': -4.359386682436024,\n",
        " u'SONG': -0.642554242449815,\n",
        " u'SQ': -3.791277518586479}"
       ]
      }
     ],
     "prompt_number": 353
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trans_p['FEAT'], trans_p['FEAT-'], trans_p['FEAT&']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 354,
       "text": [
        "({'ARTIST': 0.0}, {'ARTIST': 0.0}, {'ARTIST': 0.0})"
       ]
      }
     ],
     "prompt_number": 354
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trans_p['EQ-']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 355,
       "text": [
        "{u'ARTIST': -0.7043390294214003,\n",
        " u'FEAT': -4.465216095524041,\n",
        " u'OTHER': -1.1685222580044154,\n",
        " u'SONG': -1.6969770974107259}"
       ]
      }
     ],
     "prompt_number": 355
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trans_p['SONG&'], trans_p['ARTIST&'], trans_p['OTHER&']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 356,
       "text": [
        "({u'JOIN': -2.940122297069784, u'SONG': -0.05430758417675208},\n",
        " {u'ARTIST': -0.013336182276518269, u'JOIN': -4.323935146197783},\n",
        " {u'JOIN': -3.77377332896395, u'OTHER': -0.023233053540057887})"
       ]
      }
     ],
     "prompt_number": 356
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_p"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 357,
       "text": [
        "{u'ARTIST': -1.3699375554065447,\n",
        " u'FEAT': -4.5861990468034906,\n",
        " u'JOIN': -3.774209261966419,\n",
        " u'OTHER': -1.4905368640308674,\n",
        " u'SEP': -2.1892054584395373,\n",
        " u'SONG': -0.9795686746643757}"
       ]
      }
     ],
     "prompt_number": 357
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "initial_other = pickle.load(open('other_dict.p', 'rb'))\n",
      "emit_p['OTHER'] = token_dist(infile='video_tag0211.txt', tag_ind=3, stag='OTHER', \n",
      "                             basic=initial_other, logalize=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 158
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "initial_song = pickle.load(open('song_dict.p', 'rb'))\n",
      "emit_p['SONG'] = token_dist(infile='video_tag0211.txt', tag_ind=3, stag='SONG', \n",
      "                            basic=initial_song, logalize=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 159
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "initial_artist = pickle.load(open('artist_dict.p', 'rb'))\n",
      "emit_p['ARTIST'] = token_dist(infile='video_tag0211.txt', tag_ind=3, stag='ARTIST', \n",
      "                              basic=initial_artist, logalize=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sep_dict = token_dist(infile='video_tag0211.txt', tag_ind=3, stag='SEP',\n",
      "                           logalize=True)\n",
      "emit_p['SEP'] = {}\n",
      "for key in sep_dict:\n",
      "    value = sep_dict[key]\n",
      "    if value >  -8:\n",
      "        emit_p['SEP'][key] = value\n",
      "emit_p['SEP']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "{u'&': -6.318139157212705,\n",
        " u'+': -5.749327119180375,\n",
        " u',': -4.380561315953104,\n",
        " u'-': -0.19635176505679736,\n",
        " u'.': -5.000784204509368,\n",
        " u'/': -4.553490391462398,\n",
        " u':': -2.6124850204381573,\n",
        " u';': -6.195444715918059,\n",
        " u'@': -5.61249553316498,\n",
        " u'_': -6.36408904740456,\n",
        " u'perform': -6.309197219837043,\n",
        " u'sing': -5.429779810027727,\n",
        " u'|': -3.463891116708589,\n",
        " u'~': -4.144681141012306,\n",
        " u'\\u3000': -5.147345473033016,\n",
        " u'\\uff5e': -6.985280144025725}"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "join_dict = token_dist(infile='video_tag0211.txt', tag_ind=3, stag='JOIN',\n",
      "                            logalize=True)\n",
      "emit_p['JOIN'] = {}\n",
      "for key in join_dict:\n",
      "    value = join_dict[key]\n",
      "    if value >  -8:\n",
      "        emit_p['JOIN'][key] = value\n",
      "emit_p['JOIN']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "{u'&': -1.1402910174845775,\n",
        " u'+': -2.8881575790437473,\n",
        " u',': -1.564779748715293,\n",
        " u'-': -2.776700510666272,\n",
        " u'/': -1.989162112241054,\n",
        " u';': -6.788765565926576,\n",
        " u'and': -1.964689628378623,\n",
        " u'vs': -2.9987679920148813,\n",
        " u'with': -4.246965390332005,\n",
        " u'|': -4.651800180108554,\n",
        " u'\\u2665': -6.5845645999513644}"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feat_dict = token_dist(infile='video_tag0211.txt', tag_ind=3, stag='FEAT',\n",
      "                            logalize=True)\n",
      "emit_p['FEAT'] = {}\n",
      "for key in feat_dict:\n",
      "    value = feat_dict[key]\n",
      "    if value >  -8:\n",
      "        emit_p['FEAT'][key] = value\n",
      "emit_p['FEAT']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "{u'by': -0.9486068972173265, u'ft': -0.48984749304634323}"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['OTHER'][stem(u'romanization')] = -7.90123812058684\n",
      "emit_p['ARTIST'][stem(u'romanization')] = -13\n",
      "emit_p['SONG'][stem(u'romanization')] = -13\n",
      "emit_p['OTHER'][stem(u'KOR')] = -10\n",
      "emit_p['ARTIST'][stem(u'KOR')] = -15\n",
      "emit_p['SONG'][stem(u'KOR')] = -16\n",
      "emit_p['OTHER'][stem(u'ROM')] = -8.7\n",
      "emit_p['ARTIST'][stem(u'ROM')] = -15\n",
      "emit_p['SONG'][stem(u'ROM')] = -16\n",
      "emit_p['SEP'][u\"'\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 407,
       "text": [
        "-20"
       ]
      }
     ],
     "prompt_number": 407
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['OTHER'][stem(u'Official')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "-4.026184794441992"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['SONG'].get(stem(u'Oficial'), 'haha'),emit_p['ARTIST'].get(stem(u'Oficial'),'haha'), emit_p['OTHER'].get(stem(u'Oficial'), 'haha')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 51,
       "text": [
        "(-13.985576020877183, -7.884898612176974, 'haha')"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tag_title(inpath='result100.txt', outpath='tt100.txt', \n",
      "          info_ind=[0,1,4], title_ind=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 499
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tag_title(inpath='video0218.txt', outpath='video_tag0218.txt', \n",
      "          info_ind=[0,1,4], title_ind=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wweb = ur\"(?:https?://|www.)[\\w\\u00C0-\\u00FF]+(?:[./?=-][\\w\\u00C0-\\u00FF]+)*[/]?\"\n",
      "pp = re.compile(wweb)\n",
      "ttstring = u'N-Joi - Anthem.    https://itunes.apple.com/gb/album/anthem-single/id779427881?uo=4'\n",
      "re.findall(pp, ttstring)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 116,
       "text": [
        "[u'https://itunes.apple.com/gb/album/anthem-single/id779427881?uo=4']"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['SONG'][stem(u'Bass')] = -9\n",
      "emit_p['OTHER'][stem(u'Bass')] = -9\n",
      "emit_p['OTHER'][stem(u'Oficial')] = -7.884898612176974\n",
      "emit_p['ARTIST'][stem(u'Oficial')] = -15"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_quat_test = u'\"\u300c\u300e\u300a\u201c\u00ab'+u\"'\"\n",
      "for item in tt:\n",
      "    if stem(item) == u'':\n",
      "        print item"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u042c\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "u'One' in stop_set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "False"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tt = tokenize(u'Last Christmas cover by Chendy')\n",
      "#states = ['SONG', 'ARTIST', 'OTHER', 'FEAT', 'SEP', 'JOIN']\n",
      "#punc_tag(tt)\n",
      "for item in viterbi(tt, stemmer=False, details=False):\n",
      "    print item[0], item[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Last SONG\n",
        "Christmas SONG\n",
        "cover OTHER\n",
        "by FEAT\n",
        "Chendy ARTIST\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "u'\u2192', tt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "(u'\\u2192', [u'\\u300c', u'Nightcore', u'\\u300d', u'Last', u'Christmas'])"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['ARTIST'].get(stem(u'Chendy')), emit_p['SONG'].get(stem(u'Chendy')), emit_p['OTHER'].get(stem(u'Chendy'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "(None, -16.236867819483678, None)"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['ARTIST'][stem(u'\u0412\u043e\u0440\u043e\u0431\u044c\u0435\u0432\u0430')] = -14\n",
      "emit_p['SONG'][stem(u'\u0412\u043e\u0440\u043e\u0431\u044c\u0435\u0432\u0430')] = -20\n",
      "emit_p['OTHER'][stem(u'\u0421\u043b\u0435\u043f\u044b\u0435')] = -14\n",
      "emit_p['ARTIST'][stem(u'\u043f\u0440\u043e\u0441\u043b\u0443\u0448\u0438\u0432\u0430\u043d\u0438\u044f')] = -20\n",
      "emit_p['OTHER'][stem(u'\u043f\u0440\u043e\u0441\u043b\u0443\u0448\u0438\u0432\u0430\u043d\u0438\u044f')] = -14.8\n",
      "emit_p['FEAT'][stem(u'Learn')] = -8\n",
      "emit_p['FEAT'][stem(u'perform')] = -8\n",
      "emit_p['SONG'][stem(u'week')] = -9.5\n",
      "emit_p['JOIN'][stem(u':')] = -8\n",
      "emit_p['OTHER'][stem(u'AMAZING')] = -8.8\n",
      "emit_p['ARTIST'][stem(u'Wendorff')] = -13.4\n",
      "emit_p['OTHER'][stem(u'Wendorff')] = -20\n",
      "emit_p['OTHER'][stem(u'subscribers')] = -10\n",
      "emit_p['ARTIST'][stem(u'subscribers')] = -15\n",
      "emit_p['ARTIST'][stem(u'Giovana')] = -14\n",
      "emit_p['ARTIST'][stem(u'Chendy')] = -16\n",
      "emit_p['SONG'][stem(u'Chency')] = -20"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['OTHER'][stem(u'57th')] = -12.813797846496023\n",
      "emit_p['ARTIST'][stem(u'57th')] =  -15.237478000673713\n",
      "emit_p['FEAT'][stem(u'sings')] = -6\n",
      "emit_p['OTHER'][stem(u'ASAP')] = -9.8\n",
      "emit_p['ARTIST'][stem(u'ASAP')] = -15\n",
      "emit_p['ARTIST'][stem(u'it')] = -8\n",
      "emit_p['ARTIST'][stem(u'off')] = -8.5\n",
      "emit_p['OTHER'][stem(u'behind')] = -9\n",
      "emit_p['ARTIST'][stem(u'Breves')] = -12\n",
      "emit_p['ARTIST'][stem(u'miss')] = -6.5\n",
      "emit_p['SONG'][stem(u'A')] = -5\n",
      "emit_p['OTHER'][stem(u'Class')] = -9\n",
      "emit_p['OTHER'][stem(u'hot')] = -9\n",
      "emit_p['SONG'][stem(u'Hush')] = -9\n",
      "emit_p['ARTIST'][stem(u'Hush')] = -13\n",
      "emit_p['ARTIST'][stem(u'\ubbf8\uc4f0\uc5d0\uc774')] = -11.499808382390345\n",
      "emit_p['OTHER'][stem(u'\ubbf8\uc4f0\uc5d0\uc774')] = -20\n",
      "emit_p['SONG'][stem(u'\ud5c8\uc26c')] = -12.672528643212177\n",
      "emit_p['OTHER'][stem(u'\ud5c8\uc26c')] = -20\n",
      "emit_p['OTHER'][stem(u'title')] = -9\n",
      "emit_p['OTHER'][stem(u'Factor')] = -9.5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['OTHER'][stem(u'\u5b63')] = -8\n",
      "emit_p['OTHER'][stem(u'\u4e00')] = -9\n",
      "emit_p['SONG'][stem(u'\u9078')] = -10.5\n",
      "emit_p['OTHER'][stem(u'eggs')] = -9.732146464741351\n",
      "emit_p['ARTIST'][stem(u'Glee')] = -6.448427287152667\n",
      "emit_p['OTHER'][stem(u'Glee')] = -11.1\n",
      "emit_p['OTHER'][stem(u'LPS')] = -15.9\n",
      "emit_p['ARTIST'][stem(u'LPS')] = -8.7\n",
      "emit_p['OTHER'][stem(u'3000')] = -10.6\n",
      "emit_p['OTHER'][stem(u'entry')] = -10.4"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['SONG'][stem(u'World')] = -6.188684861962481\n",
      "emit_p['OTHER'][stem(u'World')] = -7\n",
      "emit_p['OTHER'][stem(u'Beauty')] = -8\n",
      "emit_p['OTHER'][stem(u'Interlude')] = -7\n",
      "emit_p['OTHER'][stem(u'Prelude')] = -7\n",
      "emit_p['SONG'][stem(u'Opera')] = -8.7\n",
      "emit_p['SONG'][stem(u'Roundtable')] = -10\n",
      "emit_p['SONG'][stem(u'Rival')] = -11\n",
      "emit_p['OTHER'][stem(u'Warriors')] = -13\n",
      "emit_p['ARTIST'][stem(u'Warriors')] = -12\n",
      "emit_p['OTHER'][stem(u'Collaboration')] = -10\n",
      "emit_p['ARTIST'][stem(u'Radioactive')] = -20\n",
      "emit_p['OTHER'][stem(u'Grammys')] = -9.5\n",
      "emit_p['ARTIST'][stem(u'Grammys')] = -13\n",
      "emit_p['ARTIST'][stem(u'Demons')] = -15\n",
      "emit_p['ARTIST'][stem(u'Fan')] = -9\n",
      "emit_p['OTHER'][stem(u'Fan')] = -8\n",
      "emit_p['SONG'][stem(u'REPUTASYON')] = -15\n",
      "emit_p['OTHER'][stem(u'REPUTASYON')] = -20"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['OTHER'][stem(u'Voice')] = -8"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['ARTIST'][stem(u'Novocaine')] = -15\n",
      "emit_p['SONG'][stem(u'Sugar')] = -6\n",
      "emit_p['SONG'][stem(u'Chop')] = -8.5\n",
      "emit_p['OTHER'][stem(u'Chop')] = -10.5\n",
      "emit_p['SONG'][stem(u'Suey')] = -10.5\n",
      "emit_p['OTHER'][stem(u'Suey')] = -12.3\n",
      "emit_p['SONG'][stem(u'B.Y.O.B.')] = -12\n",
      "emit_p['ARTIST'][stem(u'B.Y.O.B.')] = -20\n",
      "emit_p['ARTIST'][stem(u'Voice')] = -11\n",
      "emit_p['OTHER'][stem(u'Voice')] = -10\n",
      "emit_p['OTHER'][stem(u'Blind')] = -8.5\n",
      "emit_p['OTHER'][stem(u'Mean')] = -9\n",
      "emit_p['OTHER'][stem(u'Color')] = -7"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['OTHER'][stem(u'Jeep')] = -12.5\n",
      "emit_p['OTHER'][stem(u'FIAT')] = -13.3\n",
      "emit_p['OTHER'][stem(u'ThisBudsForYou')] = -17\n",
      "emit_p['OTHER'][stem(u'RealStrength')] = -17\n",
      "emit_p['SONG'][stem(u'7/11')] = -11.4\n",
      "emit_p['SONG'][stem(u'Flawless')] = -6\n",
      "emit_p['ARTIST'][stem(u'Gaga')] = - 7.4\n",
      "emit_p['OTHER'][stem(u'Gaga')] = - 15\n",
      "emit_p['OTHER'][stem(u'National')] = -8.5\n",
      "emit_p['OTHER'][stem(u'Kinder')] = -11\n",
      "emit_p['OTHER'][stem(u'A')] = -5\n",
      "emit_p['OTHER'][stem(u'Crafted')] = -9\n",
      "emit_p['OTHER'][stem(u'Using')] = -10\n",
      "emit_p['OTHER'][stem(u'Decisions')] = -11\n",
      "emit_p['SONG'][stem(u'Centuries')] = -7\n",
      "emit_p['ARTIST'][stem(u'Immortals')] = -13\n",
      "emit_p['SONG'][stem(u'Immortals')] = -9.2\n",
      "emit_p['SONG'][stem(u'Irresistible')] = -9\n",
      "emit_p['ARTIST'][stem(u'Uma')] = -13\n",
      "emit_p['SONG'][stem(u'Uma')] = -10\n",
      "emit_p['ARTIST'][stem(u'Thurman')] = -14\n",
      "emit_p['SONG'][stem(u'Thurman')] = -12\n",
      "emit_p['SONG'][stem(u'Beauty')] = -6\n",
      "emit_p['OTHER'][stem(u'Dance')] = -7\n",
      "emit_p['SONG'][stem(u'Phoenix')] = -7\n",
      "emit_p['SONG'][stem(u'Jet')] = -8"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['OTHER'][stem(u'Surprise')] = -9.5\n",
      "emit_p['OTHER'][stem(u'Cars')] = -8.5\n",
      "emit_p['OTHER'][stem(u'Marvel')] = -8\n",
      "emit_p['OTHER'][stem(u'Spider')] = -9.5\n",
      "emit_p['OTHER'][stem(u'PARTY')] = -7.5\n",
      "emit_p['OTHER'][stem(u'Kitty')] = -10\n",
      "emit_p['OTHER'][stem(u'Hello')] = -10\n",
      "emit_p['ARTIST'][stem(u'Coldplay')] = -7.87\n",
      "emit_p['OTHER'][stem(u'Coldplay')] = -11.25\n",
      "emit_p['ARTIST'][stem(u'MattyBRaps')] = -10.281650943072453\n",
      "emit_p['OTHER'][stem(u'MattyBRaps')] = -15\n",
      "emit_p['ARTIST'][stem(u'Commercial')] = -15.2\n",
      "emit_p['OTHER'][stem(u'Commercial')] = -9\n",
      "emit_p['SONG'][stem(u'song')] = -6\n",
      "emit_p['ARTIST'][stem(u'FROZEN')] = -9\n",
      "emit_p['OTHER'][stem(u'Sequence')] = -10.1\n",
      "emit_p['OTHER'][stem(u'Battles')] = - 8\n",
      "emit_p['OTHER'][stem(u'History')] = -9.2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['SONG'][stem(u'1979')] = -10\n",
      "emit_p['OTHER'][stem(u'Pumpkins')] = -12.5\n",
      "emit_p['ARTIST'][stem(u'Pumpkins')] = -9.04\n",
      "emit_p['OTHER'][stem(u'Smashing')] = -11\n",
      "emit_p['ARTIST'][stem(u'Smashing')] = -6.8\n",
      "emit_p['OTHER'][stem(u'Chopin')] = -12\n",
      "emit_p['ARTIST'][stem(u'Chopin')] = -8.5\n",
      "emit_p['OTHER'][stem(u'Nu')] = -9.5\n",
      "emit_p['OTHER'][stem(u'Disco')] = -8\n",
      "emit_p['OTHER'][stem(u'Deep')] = -8\n",
      "emit_p['SONG'][stem(u'Sound')] = -7\n",
      "emit_p['OTHER'][stem(u'Beyond')] = -8.5\n",
      "emit_p['OTHER'][stem(u'And')] = -6.1\n",
      "emit_p['OTHER'][stem(u'The')] = -3.4\n",
      "emit_p['OTHER'][stem(u'\uac15\ub0a8\uc2a4\ud0c0\uc77c')] = -17.6\n",
      "emit_p['SONG'][stem(u'\uac15\ub0a8\uc2a4\ud0c0\uc77c')] = -10.6\n",
      "emit_p['SONG'][stem(u'GANGNAM')] = -8.7\n",
      "emit_p['OTHER'][stem(u'GANGNAM')] = -15\n",
      "emit_p['ARTIST'][stem(u'Azalea')] = -8\n",
      "emit_p['OTHER'][stem(u'Azalea')] = -15\n",
      "emit_p['ARTIST'][stem(u'Episode')] = -15\n",
      "emit_p['OTHER'][stem(u'Episode')] = -7.03\n",
      "emit_p['ARTIST'][stem(u'Legend')] = -8\n",
      "emit_p['OTHER'][stem(u'Legend')] = -9\n",
      "emit_p['OTHER'][stem(u'how')] = -9\n",
      "emit_p['OTHER'][stem(u'to')] = -6\n",
      "emit_p['ARTIST'][stem(u'dance')] = -12.5\n",
      "emit_p['OTHER'][stem(u'dance')] = -6.414\n",
      "emit_p['ARTIST'][stem(u'by')] = -13\n",
      "emit_p['OTHER'][stem(u'more')] = -7\n",
      "emit_p['OTHER'][stem(u'lots')] = -10\n",
      "emit_p['OTHER'][stem(u'kids')] = -7\n",
      "emit_p['OTHER'][stem(u'pool')] = -9\n",
      "emit_p['OTHER'][stem(u'balls')] = -8\n",
      "emit_p['ARTIST'][stem(u'collections')] = -15\n",
      "emit_p['OTHER'][stem(u'collections')] = -7.5\n",
      "emit_p['SONG'][stem(u'collections')] = -13\n",
      "emit_p['OTHER'][stem(u'\u0421\u0435\u0440\u0438\u044f')] = -12\n",
      "emit_p['ARTIST'][stem(u'\u0421\u0435\u0440\u0438\u044f')] = -15\n",
      "emit_p['ARTIST'][stem(u'Popular')] = -15\n",
      "emit_p['OTHER'][stem(u'Popular')] = -9.7\n",
      "emit_p['ARTIST'][stem(u'5')] = -6\n",
      "emit_p['SONG'][stem(u'Maps')] = -9\n",
      "emit_p['ARTIST'][stem(u'Gente')] = -9\n",
      "emit_p['ARTIST'][stem(u'De')] = -5.5\n",
      "emit_p['ARTIST'][stem(u'Zona')] = -10.5\n",
      "emit_p['OTHER'][stem(u'minutes')] = -7.5\n",
      "emit_p['OTHER'][stem(u'54')] = -9\n",
      "emit_p['OTHER'][stem(u'Plus')]= -9\n",
      "emit_p['ARTIST'][stem(u'Magic')] = -7.5\n",
      "emit_p['OTHER'][stem(u'!')] = -6\n",
      "emit_p['ARTIST'][stem(u'!')] = -6.5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['SONG'][stem(u'Sonata')] = -16\n",
      "emit_p['ARTIST'][stem(u'Sonata')] = -12\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 473
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit viterbi(tt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1000 loops, best of 3: 861 \u00b5s per loop\n"
       ]
      }
     ],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['SONG'].get(stem(u'-'), 'haha'), \\\n",
      "emit_p['ARTIST'].get(stem(u'-'), 'haha'), \\\n",
      "emit_p['OTHER'].get(stem(u'-'), 'haha')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 162,
       "text": [
        "('haha', -7.0625144172917205, -10.103563691119072)"
       ]
      }
     ],
     "prompt_number": 162
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(emit_p, open('emit_p.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(trans_p, open('tran_p.p', 'wb'))\n",
      "pickle.dump(start_p, open('start_p.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 167
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_file_length('video_tag0204.txt'), get_file_length('video_0204.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 267,
       "text": [
        "(1990250, 1990250)"
       ]
      }
     ],
     "prompt_number": 267
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "retrain('retrain_video_tag.txt', start_p, tran_p, emit_p, 'retrain5_video_tag.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 717
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cjk_scripts = ur'[\\u4E00-\\u9FFF]|[\\u30A0-\\u30FF]+|[\\u3040-\\u309F]+|[\\uAC00-\\uD7AF]+'\n",
      "pp = re.compile(cjk_scripts)\n",
      "for key in emit_p['OTHER']:\n",
      "    if re.search(cjk_scripts, key):\n",
      "        continue\n",
      "    diff = emit_p['ARTIST'].get(key, -100) - \\\n",
      "           emit_p['SONG'].get(key, -100)\n",
      "    if diff > 3:\n",
      "        if emit_p['OTHER'][key] < -7.3:\n",
      "            emit_p['OTHER'][key] = -20\n",
      "\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 370
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "other_dict = train_other('retrain6_tag0-42.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['OTHER'][stem(u'subtitle')]= -6.0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "other_dict.get(stem(u'subtitle'), 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 80,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emit_p['OTHER'][stem(u'Piano')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 257,
       "text": [
        "-5.601811395708271"
       ]
      }
     ],
     "prompt_number": 257
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "write_dict(other_dict, 'other_dict0_42.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(other_dict, open('other_dict0_42.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myrr = train_other('result_tag150.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 317
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "write_dict(myrr, 'myrr_dict.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 193
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tag_video('video30-42.txt', tag_dict, '')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10000 loops, best of 3: 51.3 \u00b5s per loop\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rr = file_to_dict('video_feat100.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "retag('video_feat100.txt', 'result_feat100.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "retag('video_tag150.txt', 'result_tag150.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "retag('video_end_q110.txt', 'result_quat110.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count = 1\n",
      "result = []\n",
      "token_s = u\"Tokyo\"\n",
      "pp = re.compile(token_s, re.IGNORECASE)\n",
      "with open('video_tag50000.txt', 'rb') as f:\n",
      "    for line in f:\n",
      "        record = line.decode('utf-8').strip('\\n')\n",
      "        entries = record.split('<SEP>')\n",
      "        video = entries[0]\n",
      "        entry = entries[3]\n",
      "        tokentag_list = entry.split('<and>')\n",
      "        for tokentag in tokentag_list:\n",
      "            twins = tokentag.split('<tag>')\n",
      "            if len(twins) != 2:\n",
      "                break\n",
      "            token = twins[0]\n",
      "            tag = twins[1]\n",
      "            if re.search(pp, token):\n",
      "                if tag == 'ARTIST':\n",
      "                    result.append(count)\n",
      "                    break\n",
      "        count += 1\n",
      "\n",
      "for item in result:\n",
      "    print item\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "946\n",
        "1824\n",
        "3332\n",
        "4493\n",
        "4614\n",
        "8884\n",
        "12937\n",
        "13357\n",
        "14030\n",
        "16382\n",
        "19699\n"
       ]
      }
     ],
     "prompt_number": 458
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_file_length('video_entity0225.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "2936887"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('video_entity0225.txt', 'rb') as f:\n",
      "    for line in f:\n",
      "        line = line.decode('utf-8').strip()\n",
      "        entries = line.split('<SEP>')\n",
      "        if entries[0] == u'None':\n",
      "            print entries"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    }
   ],
   "metadata": {}
  }
 ]
}